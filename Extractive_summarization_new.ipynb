{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1JLQkSkvryHHAXKBZg0D2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VodnalaSrujana004/Text_summarization_project/blob/main/Extractive_summarization_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlGNwddEExGj"
      },
      "outputs": [],
      "source": [
        "#Given some sample text collected from article about text summarization\n",
        "text=\"Text summarization is the creation of a short, accurate, and fluent summary of a longer text document. Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online. This could help to discover relevant information and to consume relevant information faster. Consider the internet, which is made up of web pages, news stories, status updates, blogs, and many other things. Because the data is unstructured, the best we can do is perform search and glance over the results. Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need. We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods.To create the new summary, extractive text summarization selects phrases and sentences from the original document. Techniques include rating the importance of phrases in order to select just those that are most essential to the source's meaning. To capture the meaning of the source content, abstractive text summarization entails creating whole new words and sentences. This is a more difficult strategy, but it is also the one that humans will employ in the end. The content of the original document is selected and compressed using traditional methods. Most effective text summarizing methods are extractive because they are easier to implement, whereas abstractive alternatives offer the promise of more universal solutions.The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization. We (humans) are often good at this sort of assignment since it necessitates first comprehending the content of the original material and then distilling the meaning and capturing key features in the new description. The goal of automatic summarizing studies is to create approaches that allow a computer to generate summaries that closely resemble those produced by humans. Generating words and phrases that convey the meaning of the source content is not enough. The summary should be factual and read as if it were a separate paper.After generation of the directed total graph, G, a community detection algorithm has been executed on the graph using the Infomap tool. Infomap [68] optimizes the map equation, which takes advantage of the information theoretic duality between compressing data and recognizing and retrieving relevant patterns or structures within the data. Using a network-based clustering technique [68], communities are identified which represent sets of distinct tweet IDs. Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary.Based on the out-degree of the directed total graph, G, a summary has been generated. In the graph, a higher out-degree of a node implies more information going out from the node. From the Infomap output for each module, a representative set of nodes or tweets can be identified which are similar in nature. Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary.As in the experimental dataset, microblogging datasets such as the Twitter dataset are considered. The maximum length of a tweet is 140 characters, so it can be considered that tweets having maximum or near-maximum length are more informative. So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing spacy library\n",
        "\n",
        "# Upgrade spaCy to the latest version\n",
        "!pip install -U spacy\n",
        "\n",
        "# Download the English model for spaCy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3QYR3EBHRmu",
        "outputId": "49004adc-2a9f-4213-ba6d-27b4fd4d3890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the spaCy library for natural language processing tasks\n",
        "import spacy\n",
        "\n",
        "# Importing the list of English stop words from spaCy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "# Importing the punctuation characters from the string module\n",
        "from string import punctuation\n"
      ],
      "metadata": {
        "id": "9djSzRpqJma9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of stop words using spaCy's default stop word set\n",
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "UpDhV88yJ7D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')  # Load the English model for processing text"
      ],
      "metadata": {
        "id": "-CFQQqvuKEkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)  # Process the text with the spaCy model to create a Doc object"
      ],
      "metadata": {
        "id": "rEMwI8CfKOuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]  # Extracting the text of each token in the processed document\n",
        "print(tokens)  # Printing the list of tokens extracted from the document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWjp_58dKYsH",
        "outputId": "69f0fc62-7505-44c8-e7f7-6bcd2a8843c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Text', 'summarization', 'is', 'the', 'creation', 'of', 'a', 'short', ',', 'accurate', ',', 'and', 'fluent', 'summary', 'of', 'a', 'longer', 'text', 'document', '.', 'Automatic', 'text', 'summarization', 'methods', 'are', 'greatly', 'needed', 'to', 'address', 'the', 'ever', '-', 'growing', 'amount', 'of', 'text', 'data', 'available', 'online', '.', 'This', 'could', 'help', 'to', 'discover', 'relevant', 'information', 'and', 'to', 'consume', 'relevant', 'information', 'faster', '.', 'Consider', 'the', 'internet', ',', 'which', 'is', 'made', 'up', 'of', 'web', 'pages', ',', 'news', 'stories', ',', 'status', 'updates', ',', 'blogs', ',', 'and', 'many', 'other', 'things', '.', 'Because', 'the', 'data', 'is', 'unstructured', ',', 'the', 'best', 'we', 'can', 'do', 'is', 'perform', 'search', 'and', 'glance', 'over', 'the', 'results', '.', 'Much', 'of', 'this', 'text', 'material', 'has', 'to', 'be', 'reduced', 'to', 'shorter', ',', 'focused', 'summaries', 'that', 'capture', 'the', 'important', 'elements', ',', 'so', 'we', 'can', 'explore', 'it', 'more', 'easily', 'and', 'to', 'ensure', 'that', 'the', 'summaries', 'include', 'the', 'information', 'we', 'need', '.', 'We', 'need', 'automatic', 'text', 'summarization', 'tools', 'for', 'the', 'below', 'mentioned', 'reasons:1.Summaries', 'help', 'you', 'save', 'time', 'by', 'reducing', 'the', 'amount', 'of', 'time', 'you', 'spend', 'reading.2.Summaries', 'aid', 'in', 'the', 'selection', 'of', 'documents', 'when', 'conducting', 'research.3.The', 'efficacy', 'of', 'indexing', 'is', 'improved', 'by', 'automatic', 'summarization.4.Human', 'summarizers', 'are', 'more', 'prejudiced', 'than', 'automatic', 'summarizing', 'techniques.5.Because', 'they', 'give', 'individualized', 'information', ',', 'personalized', 'summaries', 'are', 'important', 'in', 'question', '-', 'answering', 'systems.6.Commercial', 'abstract', 'services', 'can', 'enhance', 'the', 'volume', 'of', 'texts', 'they', 'can', 'handle', 'by', 'using', 'automatic', 'or', 'semi', '-', 'automatic', 'summarizing', 'systemsThere', 'are', 'two', 'main', 'approaches', 'to', 'summarizing', 'text', 'documents:1.extractive', 'methods,2.abstractive', 'methods', '.', 'To', 'create', 'the', 'new', 'summary', ',', 'extractive', 'text', 'summarization', 'selects', 'phrases', 'and', 'sentences', 'from', 'the', 'original', 'document', '.', 'Techniques', 'include', 'rating', 'the', 'importance', 'of', 'phrases', 'in', 'order', 'to', 'select', 'just', 'those', 'that', 'are', 'most', 'essential', 'to', 'the', 'source', \"'s\", 'meaning', '.', 'To', 'capture', 'the', 'meaning', 'of', 'the', 'source', 'content', ',', 'abstractive', 'text', 'summarization', 'entails', 'creating', 'whole', 'new', 'words', 'and', 'sentences', '.', 'This', 'is', 'a', 'more', 'difficult', 'strategy', ',', 'but', 'it', 'is', 'also', 'the', 'one', 'that', 'humans', 'will', 'employ', 'in', 'the', 'end', '.', 'The', 'content', 'of', 'the', 'original', 'document', 'is', 'selected', 'and', 'compressed', 'using', 'traditional', 'methods', '.', 'Most', 'effective', 'text', 'summarizing', 'methods', 'are', 'extractive', 'because', 'they', 'are', 'easier', 'to', 'implement', ',', 'whereas', 'abstractive', 'alternatives', 'offer', 'the', 'promise', 'of', 'more', 'universal', 'solutions', '.', 'The', 'act', 'of', 'constructing', 'a', 'concise', 'and', 'coherent', 'version', 'of', 'a', 'lengthier', 'document', 'is', 'known', 'as', 'automatic', 'text', 'summarizing', ',', 'or', 'simply', 'text', 'summarization', '.', 'We', '(', 'humans', ')', 'are', 'often', 'good', 'at', 'this', 'sort', 'of', 'assignment', 'since', 'it', 'necessitates', 'first', 'comprehending', 'the', 'content', 'of', 'the', 'original', 'material', 'and', 'then', 'distilling', 'the', 'meaning', 'and', 'capturing', 'key', 'features', 'in', 'the', 'new', 'description', '.', 'The', 'goal', 'of', 'automatic', 'summarizing', 'studies', 'is', 'to', 'create', 'approaches', 'that', 'allow', 'a', 'computer', 'to', 'generate', 'summaries', 'that', 'closely', 'resemble', 'those', 'produced', 'by', 'humans', '.', 'Generating', 'words', 'and', 'phrases', 'that', 'convey', 'the', 'meaning', 'of', 'the', 'source', 'content', 'is', 'not', 'enough', '.', 'The', 'summary', 'should', 'be', 'factual', 'and', 'read', 'as', 'if', 'it', 'were', 'a', 'separate', 'paper', '.', 'After', 'generation', 'of', 'the', 'directed', 'total', 'graph', ',', 'G', ',', 'a', 'community', 'detection', 'algorithm', 'has', 'been', 'executed', 'on', 'the', 'graph', 'using', 'the', 'Infomap', 'tool', '.', 'Infomap', '[', '68', ']', 'optimizes', 'the', 'map', 'equation', ',', 'which', 'takes', 'advantage', 'of', 'the', 'information', 'theoretic', 'duality', 'between', 'compressing', 'data', 'and', 'recognizing', 'and', 'retrieving', 'relevant', 'patterns', 'or', 'structures', 'within', 'the', 'data', '.', 'Using', 'a', 'network', '-', 'based', 'clustering', 'technique', '[', '68', ']', ',', 'communities', 'are', 'identified', 'which', 'represent', 'sets', 'of', 'distinct', 'tweet', 'IDs', '.', 'Finally', 'distinct', 'tweets', 'are', 'identified', 'mapping', 'the', 'tweet', 'ID', 'into', 'tweets', 'and', 'summaries', 'are', 'generated', 'considering', 'one', 'representative', 'tweet', 'from', 'each', 'community', ',', 'which', 'is', 'named', 'total', 'summary', '.', 'Based', 'on', 'the', 'out', '-', 'degree', 'of', 'the', 'directed', 'total', 'graph', ',', 'G', ',', 'a', 'summary', 'has', 'been', 'generated', '.', 'In', 'the', 'graph', ',', 'a', 'higher', 'out', '-', 'degree', 'of', 'a', 'node', 'implies', 'more', 'information', 'going', 'out', 'from', 'the', 'node', '.', 'From', 'the', 'Infomap', 'output', 'for', 'each', 'module', ',', 'a', 'representative', 'set', 'of', 'nodes', 'or', 'tweets', 'can', 'be', 'identified', 'which', 'are', 'similar', 'in', 'nature', '.', 'Instead', 'of', 'considering', 'module', 'representatives', ',', 'as', 'a', 'summary', 'entry', 'the', 'highest', '-', 'degree', 'node', 'has', 'been', 'chosen', 'from', 'each', 'set', 'corresponding', 'to', 'modules', 'and', 'a', 'summary', 'file', 'has', 'been', 'generated', 'which', 'is', 'named', 'total', 'degree', 'summary', '.', 'As', 'in', 'the', 'experimental', 'dataset', ',', 'microblogging', 'datasets', 'such', 'as', 'the', 'Twitter', 'dataset', 'are', 'considered', '.', 'The', 'maximum', 'length', 'of', 'a', 'tweet', 'is', '140', 'characters', ',', 'so', 'it', 'can', 'be', 'considered', 'that', 'tweets', 'having', 'maximum', 'or', 'near', '-', 'maximum', 'length', 'are', 'more', 'informative', '.', 'So', ',', 'instead', 'of', 'considering', 'a', 'module', 'representative', 'as', 'a', 'summary', 'entry', ',', 'the', 'highest', '-', 'length', 'node', 'has', 'been', 'chosen', 'from', 'each', 'set', 'corresponding', 'to', 'modules', 'and', 'a', 'summary', 'file', 'has', 'been', 'generated', 'which', 'is', 'named', 'total', 'length', 'summary', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'  # Modify punctuation to include newline characters\n",
        "punctuation  # Use punctuation variable in further processing or output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kIlxpSBQKir9",
        "outputId": "66079e2e-4463-4d41-8ae3-b1ed47d83036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}  # Initializing a dictionary to store word frequencies\n",
        "\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:  # Filtering out stopwords\n",
        "        if word.text.lower() not in punctuation:  # Filtering out punctuation\n",
        "            if word.text not in word_frequencies.keys():  # Adding new words to word frequencies\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1  # Incrementing frequency for existing words\n"
      ],
      "metadata": {
        "id": "VkKkZ2FwKmt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies) #printing out the frequencies of each word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yufFOjnWK25G",
        "outputId": "c1b95593-fa27-4676-ce89-d61e4acd740b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Text': 1, 'summarization': 6, 'creation': 1, 'short': 1, 'accurate': 1, 'fluent': 1, 'summary': 11, 'longer': 1, 'text': 11, 'document': 4, 'Automatic': 1, 'methods': 4, 'greatly': 1, 'needed': 1, 'address': 1, 'growing': 1, 'data': 4, 'available': 1, 'online': 1, 'help': 2, 'discover': 1, 'relevant': 3, 'information': 6, 'consume': 1, 'faster': 1, 'Consider': 1, 'internet': 1, 'web': 1, 'pages': 1, 'news': 1, 'stories': 1, 'status': 1, 'updates': 1, 'blogs': 1, 'things': 1, 'unstructured': 1, 'best': 1, 'perform': 1, 'search': 1, 'glance': 1, 'results': 1, 'material': 2, 'reduced': 1, 'shorter': 1, 'focused': 1, 'summaries': 5, 'capture': 2, 'important': 2, 'elements': 1, 'explore': 1, 'easily': 1, 'ensure': 1, 'include': 2, 'need': 2, 'automatic': 7, 'tools': 1, 'mentioned': 1, 'reasons:1.Summaries': 1, 'save': 1, 'time': 2, 'reducing': 1, 'spend': 1, 'reading.2.Summaries': 1, 'aid': 1, 'selection': 1, 'documents': 1, 'conducting': 1, 'research.3.The': 1, 'efficacy': 1, 'indexing': 1, 'improved': 1, 'summarization.4.Human': 1, 'summarizers': 1, 'prejudiced': 1, 'summarizing': 6, 'techniques.5.Because': 1, 'individualized': 1, 'personalized': 1, 'question': 1, 'answering': 1, 'systems.6.Commercial': 1, 'abstract': 1, 'services': 1, 'enhance': 1, 'volume': 1, 'texts': 1, 'handle': 1, 'semi': 1, 'systemsThere': 1, 'main': 1, 'approaches': 2, 'documents:1.extractive': 1, 'methods,2.abstractive': 1, 'create': 2, 'new': 3, 'extractive': 2, 'selects': 1, 'phrases': 3, 'sentences': 2, 'original': 3, 'Techniques': 1, 'rating': 1, 'importance': 1, 'order': 1, 'select': 1, 'essential': 1, 'source': 3, 'meaning': 4, 'content': 4, 'abstractive': 2, 'entails': 1, 'creating': 1, 'words': 2, 'difficult': 1, 'strategy': 1, 'humans': 3, 'employ': 1, 'end': 1, 'selected': 1, 'compressed': 1, 'traditional': 1, 'effective': 1, 'easier': 1, 'implement': 1, 'alternatives': 1, 'offer': 1, 'promise': 1, 'universal': 1, 'solutions': 1, 'act': 1, 'constructing': 1, 'concise': 1, 'coherent': 1, 'version': 1, 'lengthier': 1, 'known': 1, 'simply': 1, 'good': 1, 'sort': 1, 'assignment': 1, 'necessitates': 1, 'comprehending': 1, 'distilling': 1, 'capturing': 1, 'key': 1, 'features': 1, 'description': 1, 'goal': 1, 'studies': 1, 'allow': 1, 'computer': 1, 'generate': 1, 'closely': 1, 'resemble': 1, 'produced': 1, 'Generating': 1, 'convey': 1, 'factual': 1, 'read': 1, 'separate': 1, 'paper': 1, 'generation': 1, 'directed': 2, 'total': 5, 'graph': 4, 'G': 2, 'community': 2, 'detection': 1, 'algorithm': 1, 'executed': 1, 'Infomap': 3, 'tool': 1, '68': 2, 'optimizes': 1, 'map': 1, 'equation': 1, 'takes': 1, 'advantage': 1, 'theoretic': 1, 'duality': 1, 'compressing': 1, 'recognizing': 1, 'retrieving': 1, 'patterns': 1, 'structures': 1, 'network': 1, 'based': 1, 'clustering': 1, 'technique': 1, 'communities': 1, 'identified': 3, 'represent': 1, 'sets': 1, 'distinct': 2, 'tweet': 4, 'IDs': 1, 'Finally': 1, 'tweets': 4, 'mapping': 1, 'ID': 1, 'generated': 4, 'considering': 3, 'representative': 3, 'named': 3, 'Based': 1, 'degree': 4, 'higher': 1, 'node': 4, 'implies': 1, 'going': 1, 'output': 1, 'module': 3, 'set': 3, 'nodes': 1, 'similar': 1, 'nature': 1, 'Instead': 1, 'representatives': 1, 'entry': 2, 'highest': 2, 'chosen': 2, 'corresponding': 2, 'modules': 2, 'file': 2, 'experimental': 1, 'dataset': 2, 'microblogging': 1, 'datasets': 1, 'Twitter': 1, 'considered': 2, 'maximum': 3, 'length': 4, '140': 1, 'characters': 1, 'having': 1, 'near': 1, 'informative': 1, 'instead': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())  # Finding the maximum frequency value from a dictionary of word frequencies."
      ],
      "metadata": {
        "id": "ynPX3wNIK_9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency #printing the maximum frequency notes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx-4ntGeLLgz",
        "outputId": "f7da8e86-4a90-41d8-c566-9f3c5fcbd44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing word frequencies to improve text processing accuracy.\n",
        "for word in word_frequencies.keys():\n",
        "  word_frequencies[word] = word_frequencies[word]/max_frequency"
      ],
      "metadata": {
        "id": "Zs1qLciiLo5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BdCWLjbLxE6",
        "outputId": "64797e1f-f8dd-4bbb-dc3e-66bc0e28fee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Text': 0.008264462809917356, 'summarization': 0.04958677685950413, 'creation': 0.008264462809917356, 'short': 0.008264462809917356, 'accurate': 0.008264462809917356, 'fluent': 0.008264462809917356, 'summary': 0.09090909090909091, 'longer': 0.008264462809917356, 'text': 0.09090909090909091, 'document': 0.03305785123966942, 'Automatic': 0.008264462809917356, 'methods': 0.03305785123966942, 'greatly': 0.008264462809917356, 'needed': 0.008264462809917356, 'address': 0.008264462809917356, 'growing': 0.008264462809917356, 'data': 0.03305785123966942, 'available': 0.008264462809917356, 'online': 0.008264462809917356, 'help': 0.01652892561983471, 'discover': 0.008264462809917356, 'relevant': 0.024793388429752063, 'information': 0.04958677685950413, 'consume': 0.008264462809917356, 'faster': 0.008264462809917356, 'Consider': 0.008264462809917356, 'internet': 0.008264462809917356, 'web': 0.008264462809917356, 'pages': 0.008264462809917356, 'news': 0.008264462809917356, 'stories': 0.008264462809917356, 'status': 0.008264462809917356, 'updates': 0.008264462809917356, 'blogs': 0.008264462809917356, 'things': 0.008264462809917356, 'unstructured': 0.008264462809917356, 'best': 0.008264462809917356, 'perform': 0.008264462809917356, 'search': 0.008264462809917356, 'glance': 0.008264462809917356, 'results': 0.008264462809917356, 'material': 0.01652892561983471, 'reduced': 0.008264462809917356, 'shorter': 0.008264462809917356, 'focused': 0.008264462809917356, 'summaries': 0.04132231404958678, 'capture': 0.01652892561983471, 'important': 0.01652892561983471, 'elements': 0.008264462809917356, 'explore': 0.008264462809917356, 'easily': 0.008264462809917356, 'ensure': 0.008264462809917356, 'include': 0.01652892561983471, 'need': 0.01652892561983471, 'automatic': 0.05785123966942149, 'tools': 0.008264462809917356, 'mentioned': 0.008264462809917356, 'reasons:1.Summaries': 0.008264462809917356, 'save': 0.008264462809917356, 'time': 0.01652892561983471, 'reducing': 0.008264462809917356, 'spend': 0.008264462809917356, 'reading.2.Summaries': 0.008264462809917356, 'aid': 0.008264462809917356, 'selection': 0.008264462809917356, 'documents': 0.008264462809917356, 'conducting': 0.008264462809917356, 'research.3.The': 0.008264462809917356, 'efficacy': 0.008264462809917356, 'indexing': 0.008264462809917356, 'improved': 0.008264462809917356, 'summarization.4.Human': 0.008264462809917356, 'summarizers': 0.008264462809917356, 'prejudiced': 0.008264462809917356, 'summarizing': 0.04958677685950413, 'techniques.5.Because': 0.008264462809917356, 'individualized': 0.008264462809917356, 'personalized': 0.008264462809917356, 'question': 0.008264462809917356, 'answering': 0.008264462809917356, 'systems.6.Commercial': 0.008264462809917356, 'abstract': 0.008264462809917356, 'services': 0.008264462809917356, 'enhance': 0.008264462809917356, 'volume': 0.008264462809917356, 'texts': 0.008264462809917356, 'handle': 0.008264462809917356, 'semi': 0.008264462809917356, 'systemsThere': 0.008264462809917356, 'main': 0.008264462809917356, 'approaches': 0.01652892561983471, 'documents:1.extractive': 0.008264462809917356, 'methods,2.abstractive': 0.008264462809917356, 'create': 0.01652892561983471, 'new': 0.024793388429752063, 'extractive': 0.01652892561983471, 'selects': 0.008264462809917356, 'phrases': 0.024793388429752063, 'sentences': 0.01652892561983471, 'original': 0.024793388429752063, 'Techniques': 0.008264462809917356, 'rating': 0.008264462809917356, 'importance': 0.008264462809917356, 'order': 0.008264462809917356, 'select': 0.008264462809917356, 'essential': 0.008264462809917356, 'source': 0.024793388429752063, 'meaning': 0.03305785123966942, 'content': 0.03305785123966942, 'abstractive': 0.01652892561983471, 'entails': 0.008264462809917356, 'creating': 0.008264462809917356, 'words': 0.01652892561983471, 'difficult': 0.008264462809917356, 'strategy': 0.008264462809917356, 'humans': 0.024793388429752063, 'employ': 0.008264462809917356, 'end': 0.008264462809917356, 'selected': 0.008264462809917356, 'compressed': 0.008264462809917356, 'traditional': 0.008264462809917356, 'effective': 0.008264462809917356, 'easier': 0.008264462809917356, 'implement': 0.008264462809917356, 'alternatives': 0.008264462809917356, 'offer': 0.008264462809917356, 'promise': 0.008264462809917356, 'universal': 0.008264462809917356, 'solutions': 0.008264462809917356, 'act': 0.008264462809917356, 'constructing': 0.008264462809917356, 'concise': 0.008264462809917356, 'coherent': 0.008264462809917356, 'version': 0.008264462809917356, 'lengthier': 0.008264462809917356, 'known': 0.008264462809917356, 'simply': 0.008264462809917356, 'good': 0.008264462809917356, 'sort': 0.008264462809917356, 'assignment': 0.008264462809917356, 'necessitates': 0.008264462809917356, 'comprehending': 0.008264462809917356, 'distilling': 0.008264462809917356, 'capturing': 0.008264462809917356, 'key': 0.008264462809917356, 'features': 0.008264462809917356, 'description': 0.008264462809917356, 'goal': 0.008264462809917356, 'studies': 0.008264462809917356, 'allow': 0.008264462809917356, 'computer': 0.008264462809917356, 'generate': 0.008264462809917356, 'closely': 0.008264462809917356, 'resemble': 0.008264462809917356, 'produced': 0.008264462809917356, 'Generating': 0.008264462809917356, 'convey': 0.008264462809917356, 'factual': 0.008264462809917356, 'read': 0.008264462809917356, 'separate': 0.008264462809917356, 'paper': 0.008264462809917356, 'generation': 0.008264462809917356, 'directed': 0.01652892561983471, 'total': 0.04132231404958678, 'graph': 0.03305785123966942, 'G': 0.01652892561983471, 'community': 0.01652892561983471, 'detection': 0.008264462809917356, 'algorithm': 0.008264462809917356, 'executed': 0.008264462809917356, 'Infomap': 0.024793388429752063, 'tool': 0.008264462809917356, '68': 0.01652892561983471, 'optimizes': 0.008264462809917356, 'map': 0.008264462809917356, 'equation': 0.008264462809917356, 'takes': 0.008264462809917356, 'advantage': 0.008264462809917356, 'theoretic': 0.008264462809917356, 'duality': 0.008264462809917356, 'compressing': 0.008264462809917356, 'recognizing': 0.008264462809917356, 'retrieving': 0.008264462809917356, 'patterns': 0.008264462809917356, 'structures': 0.008264462809917356, 'network': 0.008264462809917356, 'based': 0.008264462809917356, 'clustering': 0.008264462809917356, 'technique': 0.008264462809917356, 'communities': 0.008264462809917356, 'identified': 0.024793388429752063, 'represent': 0.008264462809917356, 'sets': 0.008264462809917356, 'distinct': 0.01652892561983471, 'tweet': 0.03305785123966942, 'IDs': 0.008264462809917356, 'Finally': 0.008264462809917356, 'tweets': 0.03305785123966942, 'mapping': 0.008264462809917356, 'ID': 0.008264462809917356, 'generated': 0.03305785123966942, 'considering': 0.024793388429752063, 'representative': 0.024793388429752063, 'named': 0.024793388429752063, 'Based': 0.008264462809917356, 'degree': 0.03305785123966942, 'higher': 0.008264462809917356, 'node': 0.03305785123966942, 'implies': 0.008264462809917356, 'going': 0.008264462809917356, 'output': 0.008264462809917356, 'module': 0.024793388429752063, 'set': 0.024793388429752063, 'nodes': 0.008264462809917356, 'similar': 0.008264462809917356, 'nature': 0.008264462809917356, 'Instead': 0.008264462809917356, 'representatives': 0.008264462809917356, 'entry': 0.01652892561983471, 'highest': 0.01652892561983471, 'chosen': 0.01652892561983471, 'corresponding': 0.01652892561983471, 'modules': 0.01652892561983471, 'file': 0.01652892561983471, 'experimental': 0.008264462809917356, 'dataset': 0.01652892561983471, 'microblogging': 0.008264462809917356, 'datasets': 0.008264462809917356, 'Twitter': 0.008264462809917356, 'considered': 0.01652892561983471, 'maximum': 0.024793388429752063, 'length': 0.03305785123966942, '140': 0.008264462809917356, 'characters': 0.008264462809917356, 'having': 0.008264462809917356, 'near': 0.008264462809917356, 'informative': 0.008264462809917356, 'instead': 0.008264462809917356}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizes the document into sentences using spaCy.\n",
        "\n",
        "sentence_tokens = [sent for sent in doc.sents]\n",
        " # Prints out the list of sentence tokens for analysis or further processing.\n",
        "\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTV1VqklL-uJ",
        "outputId": "1747f4ac-2cd7-4442-9a8f-397199c6eee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Text summarization is the creation of a short, accurate, and fluent summary of a longer text document., Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online., This could help to discover relevant information and to consume relevant information faster., Consider the internet, which is made up of web pages, news stories, status updates, blogs, and many other things., Because the data is unstructured, the best we can do is perform search and glance over the results., Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need., We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods., To create the new summary, extractive text summarization selects phrases and sentences from the original document., Techniques include rating the importance of phrases in order to select just those that are most essential to the source's meaning., To capture the meaning of the source content, abstractive text summarization entails creating whole new words and sentences., This is a more difficult strategy, but it is also the one that humans will employ in the end., The content of the original document is selected and compressed using traditional methods., Most effective text summarizing methods are extractive because they are easier to implement, whereas abstractive alternatives offer the promise of more universal solutions., The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization., We (humans) are often good at this sort of assignment since it necessitates first comprehending the content of the original material and then distilling the meaning and capturing key features in the new description., The goal of automatic summarizing studies is to create approaches that allow a computer to generate summaries that closely resemble those produced by humans., Generating words and phrases that convey the meaning of the source content is not enough., The summary should be factual and read as if it were a separate paper., After generation of the directed total graph, G, a community detection algorithm has been executed on the graph using the Infomap tool., Infomap, [68] optimizes the map equation, which takes advantage of the information theoretic duality between compressing data and recognizing and retrieving relevant patterns or structures within the data., Using a network-based clustering technique [68], communities are identified which represent sets of distinct tweet IDs., Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary., Based on the out-degree of the directed total graph, G, a summary has been generated., In the graph, a higher out-degree of a node implies more information going out from the node., From the Infomap output for each module, a representative set of nodes or tweets can be identified which are similar in nature., Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary., As in the experimental dataset, microblogging datasets such as the Twitter dataset are considered., The maximum length of a tweet is 140 characters, so it can be considered that tweets having maximum or near-maximum length are more informative., So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence_scores = {}  # Initializing a dictionary to store scores for each sentence\n",
        "for sent in sentence_tokens:  # Iterating through each sentence in the tokenized text\n",
        "  for word in sent:  # Iterating through each word in the current sentence\n",
        "    if word.text.lower() in word_frequencies.keys():  # Checking if the word exists in the word frequencies\n",
        "      if sent not in sentence_scores.keys():  # Checking if the sentence is not already in the scores dictionary\n",
        "        sentence_scores[sent] = word_frequencies[word.text.lower()]  # Assigning initial score to the sentence\n",
        "      else:\n",
        "        sentence_scores[sent] += word_frequencies[word.text.lower()]  # Updating the score for the sentence\n"
      ],
      "metadata": {
        "id": "dwmP8b4MMSqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the scores of the words/tokens\n",
        "sentence_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgV0dqz3MWOP",
        "outputId": "ef855e0d-6823-4ba8-fca8-eb36b262e315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{Text summarization is the creation of a short, accurate, and fluent summary of a longer text document.: 0.39669421487603307,\n",
              " Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online.: 0.4049586776859504,\n",
              " This could help to discover relevant information and to consume relevant information faster.: 0.19008264462809918,\n",
              " Consider the internet, which is made up of web pages, news stories, status updates, blogs, and many other things.: 0.0743801652892562,\n",
              " Because the data is unstructured, the best we can do is perform search and glance over the results.: 0.08264462809917356,\n",
              " Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need.: 0.36363636363636365,\n",
              " We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods.: 1.1239669421487597,\n",
              " To create the new summary, extractive text summarization selects phrases and sentences from the original document.: 0.396694214876033,\n",
              " Techniques include rating the importance of phrases in order to select just those that are most essential to the source's meaning.: 0.14049586776859505,\n",
              " To capture the meaning of the source content, abstractive text summarization entails creating whole new words and sentences.: 0.3388429752066115,\n",
              " This is a more difficult strategy, but it is also the one that humans will employ in the end.: 0.05785123966942149,\n",
              " The content of the original document is selected and compressed using traditional methods.: 0.1487603305785124,\n",
              " Most effective text summarizing methods are extractive because they are easier to implement, whereas abstractive alternatives offer the promise of more universal solutions.: 0.2727272727272727,\n",
              " The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization.: 0.4380165289256199,\n",
              " We (humans) are often good at this sort of assignment since it necessitates first comprehending the content of the original material and then distilling the meaning and capturing key features in the new description.: 0.2396694214876033,\n",
              " The goal of automatic summarizing studies is to create approaches that allow a computer to generate summaries that closely resemble those produced by humans.: 0.2727272727272727,\n",
              " Generating words and phrases that convey the meaning of the source content is not enough.: 0.14049586776859505,\n",
              " The summary should be factual and read as if it were a separate paper.: 0.12396694214876033,\n",
              " After generation of the directed total graph, G, a community detection algorithm has been executed on the graph using the Infomap tool.: 0.18181818181818182,\n",
              " [68] optimizes the map equation, which takes advantage of the information theoretic duality between compressing data and recognizing and retrieving relevant patterns or structures within the data.: 0.25619834710743805,\n",
              " Using a network-based clustering technique [68], communities are identified which represent sets of distinct tweet IDs.: 0.1487603305785124,\n",
              " Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary.: 0.47933884297520657,\n",
              " Based on the out-degree of the directed total graph, G, a summary has been generated.: 0.25619834710743805,\n",
              " In the graph, a higher out-degree of a node implies more information going out from the node.: 0.2066115702479339,\n",
              " From the Infomap output for each module, a representative set of nodes or tweets can be identified which are similar in nature.: 0.1652892561983471,\n",
              " Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary.: 0.6611570247933884,\n",
              " As in the experimental dataset, microblogging datasets such as the Twitter dataset are considered.: 0.0743801652892562,\n",
              " The maximum length of a tweet is 140 characters, so it can be considered that tweets having maximum or near-maximum length are more informative.: 0.2644628099173554,\n",
              " So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary.: 0.6776859504132231}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Importing nlargest function from heapq module for extracting top elements efficiently.\n",
        " from heapq import nlargest"
      ],
      "metadata": {
        "id": "V8dP0xR5MpiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating 30% of the length of sentence tokens for summarization\n",
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "\n",
        " # Variable storing the selected length for summarization\n",
        "select_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N3RXEgzNORJ",
        "outputId": "008d0e67-7c53-421c-fcfc-f8e8b33758b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a summary using the nlargest function with specified parameters\n",
        "summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)"
      ],
      "metadata": {
        "id": "utI_1UakNjSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL9sh72_OJLL",
        "outputId": "c957307b-8c8b-4c9a-b1f3-fd450b6d9088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods.,\n",
              " So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary.,\n",
              " Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary.,\n",
              " Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary.,\n",
              " The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization.,\n",
              " Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online.,\n",
              " Text summarization is the creation of a short, accurate, and fluent summary of a longer text document.,\n",
              " To create the new summary, extractive text summarization selects phrases and sentences from the original document.,\n",
              " Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need.]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of text tokens from 'summary' for final summarization output.\n",
        "final_summary = [word.text for word in summary]"
      ],
      "metadata": {
        "id": "CpDga-EkOTz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HdlRfLLOdKj",
        "outputId": "c9d20e2f-0f87-4744-e971-d5dc7461fa2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text summarization is the creation of a short, accurate, and fluent summary of a longer text document. Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online. This could help to discover relevant information and to consume relevant information faster. Consider the internet, which is made up of web pages, news stories, status updates, blogs, and many other things. Because the data is unstructured, the best we can do is perform search and glance over the results. Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need. We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods.To create the new summary, extractive text summarization selects phrases and sentences from the original document. Techniques include rating the importance of phrases in order to select just those that are most essential to the source's meaning. To capture the meaning of the source content, abstractive text summarization entails creating whole new words and sentences. This is a more difficult strategy, but it is also the one that humans will employ in the end. The content of the original document is selected and compressed using traditional methods. Most effective text summarizing methods are extractive because they are easier to implement, whereas abstractive alternatives offer the promise of more universal solutions.The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization. We (humans) are often good at this sort of assignment since it necessitates first comprehending the content of the original material and then distilling the meaning and capturing key features in the new description. The goal of automatic summarizing studies is to create approaches that allow a computer to generate summaries that closely resemble those produced by humans. Generating words and phrases that convey the meaning of the source content is not enough. The summary should be factual and read as if it were a separate paper.After generation of the directed total graph, G, a community detection algorithm has been executed on the graph using the Infomap tool. Infomap [68] optimizes the map equation, which takes advantage of the information theoretic duality between compressing data and recognizing and retrieving relevant patterns or structures within the data. Using a network-based clustering technique [68], communities are identified which represent sets of distinct tweet IDs. Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary.Based on the out-degree of the directed total graph, G, a summary has been generated. In the graph, a higher out-degree of a node implies more information going out from the node. From the Infomap output for each module, a representative set of nodes or tweets can be identified which are similar in nature. Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary.As in the experimental dataset, microblogging datasets such as the Twitter dataset are considered. The maximum length of a tweet is 140 characters, so it can be considered that tweets having maximum or near-maximum length are more informative. So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the extractive summary\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK5ti468OkkZ",
        "outputId": "b63d57de-5b39-4c55-a839-f73dcf4f862d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[We need automatic text summarization tools for the below mentioned reasons:1.Summaries help you save time by reducing the amount of time you spend reading.2.Summaries aid in the selection of documents when conducting research.3.The efficacy of indexing is improved by automatic summarization.4.Human summarizers are more prejudiced than automatic summarizing techniques.5.Because they give individualized information, personalized summaries are important in question-answering systems.6.Commercial abstract services can enhance the volume of texts they can handle by using automatic or semi-automatic summarizing systemsThere are two main approaches to summarizing text documents:1.extractive methods,2.abstractive methods., So, instead of considering a module representative as a summary entry, the highest-length node has been chosen from each set corresponding to modules and a summary file has been generated which is named total length summary., Instead of considering module representatives, as a summary entry the highest-degree node has been chosen from each set corresponding to modules and a summary file has been generated which is named total degree summary., Finally distinct tweets are identified mapping the tweet ID into tweets and summaries are generated considering one representative tweet from each community, which is named total summary., The act of constructing a concise and coherent version of a lengthier document is known as automatic text summarizing, or simply text summarization., Automatic text summarization methods are greatly needed to address the ever-growing amount of text data available online., Text summarization is the creation of a short, accurate, and fluent summary of a longer text document., To create the new summary, extractive text summarization selects phrases and sentences from the original document., Much of this text material has to be reduced to shorter, focused summaries that capture the important elements, so we can explore it more easily and to ensure that the summaries include the information we need.]\n"
          ]
        }
      ]
    }
  ]
}